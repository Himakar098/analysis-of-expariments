---
title: "STAT2401 Analysis of Experiments"
author: '**Practical Week 4: Solutions**'
#date: "Practical 2"
output:
  html_document:
    highlight: haddock
    # number_sections: yes
    theme: flatly
    toc: yes
  html_notebook:
    highlight: haddock
    # number_sections: yes
    theme: flatly
  pdf_document:
    toc: yes
  word_document:
    highlight: tango
    toc: yes
---

```{r echo=TRUE, include=FALSE}
knitr::opts_chunk$set(prompt=FALSE, comment=NA, message=FALSE, tidy=TRUE, error=TRUE)
```

**Aims of this session**

When you complete this session, you will be able to:

1. use *lm()*  in R to obtain the least-squares line to model a linear relationship between two continuous variables;

2. interpret the slope and intercept of a regression line;

3. understand the different information provided by the hypothesis test in a regression output in R;

4. perform hypothesis testing related to a SLR;

5. calculate the confidence intervals for the parameters in SLR.

      - To create a new chunk: Ctrl+Alt+I (Windows), Cmd+Options+I (MacOs)
      
      - Attempt this practical using this Rmd file, then rename it as your own file (use File then Save As)


Before you start, load the stored datasets provided for exercises below by running the following chunk (assuming the file  "Workshop5.Rdata" is saved in the same directory as the .Rmd  worksheet).


```{r echo=F}
print(load("Workshop5.RData"))
```

## Question 1: Simple linear regression example *meat* processing and *pH*

The data for this exercise is concerned with *meat* processing and *pH*.

A certain kind of *meat* processing may begin once the *pH* in postmortem muscle of a steer carcass has decreased 
sufficiently. To estimate the timepoint at which *pH* has dropped sufficiently, 10 steer carcasses were assigned to be 
measured for *pH* at one of five times after slaughter.  The variables in the data set are: *ph* the pH level in 
post-mortem muscle and *Time* time after slaughter in hours.


First we set the working directory, load the data, look at its structure  (the \R~command *str()* is very useful to obtain
an idea about what is stored in an object) and look at the data. You can read these data into \R, and store them as a
data frame *meat*, with the following command:

```{r }
meat = read.csv(file="meat.csv",header=TRUE)
str(meat)
meat
```


As always it is good to visualise the data. Look at the data frame by simply typing its name, *meat*.
Produce a standard scatterplot of *pH* against *Time* with

```{r }
plot(pH~Time, data=meat)
```

What are your initial impressions?

Initial impressions should be that there is a relationship between *Time* and *pH* with a seemingly exponential decay look to it.  It probably at this moment does not seem sensible to fit a straight line to these data. 

Remember we are looking to fit a linear regression to this data.  Does this seem sensible? Let's try it.

Fit a simple linear regression model to the data by:

```{r }
meat.lm = lm(pH~Time, data=meat)
summary(meat.lm)
```


The fitted model equation is given by: 

$$
\widehat{{pH}} = {6.9965} - {0.2087} \times *Time*
$$

where $6.9965$ is the intercept and $-0.2087$ is the slope, which would give the change in *pH* as *Time* is increased by $1$ unit (assuming the model is any good).

We can add the fitted line to the plot by:

```{r }
plot(pH~Time, data=meat)
abline(coef(meat.lm), col="red")
```

This doesn't look like a brilliant fit.  We could of course get a better idea from our residuals plot later.


Try the following scatterplot:

```{r }
plot(pH~log(Time), data=meat)
```

Would a linear regression seem more sensible now?

We fit a simple linear regression of *pH* on *log(Time)* in the same way we have just done.  

That looks something which would be more sensible to put a straight line through I think!  Refitting the model with *log(Time)* we get 

```{r }
meat.lm.log=lm(pH~log(Time), data=meat)
summary(meat.lm.log)
plot(pH~log(Time), data=meat)
abline(coef(meat.lm.log), col="red")
```

This now looks a much better fit.  We should be careful however to interpret the coefficients on the scale in which they are measured now. 


## Question 2: Regression Analysis of Oxygen Uptake Data


This exercise is concerned with *oxygen* uptake in the human body.
When exercising, the human body has an increased demand for oxygen that is
met by increasing heart rate. Heart rate is easy to measure, but measuring oxygen
uptake requires elaborate equipment. It is therefore of interest to see whether
oxygen uptake can be accurately predicted from heart rate.

The data for this exercise comprise measurements on heart rate (*Rate*) and
oxygen uptake (*Oxygen*) for a single individual, obtained in a laboratory at
Purdue University in the U.S. You can read these data into \R, and store them as a
data frame {oxy}, with the following command:

```{r }
oxy = read.table(file="oxygen.txt",header=T)
str(oxy)
oxy
```

Look at the data frame by simply typing its name, *oxy*. Produce a scatterplot of *Oxygen* against *Rate* with and state also your initial impressions.

```{r }
plot(Oxygen~Rate,data=oxy)
```

Initial impressions should indicate theat a straight line relationship may not be sufficient to describe the variation in *oxygen* by *Rate* however it could be that one or two points are making us think of a non-linear realtionship. 


Fitting a simple linear regression model to the data:

```{r }
oxy.lm = lm(Oxygen ~ Rate, data=oxy)
summary(oxy.lm)
```

The fitted model equation is: 
$$
\widehat{{oxygen}} = {-2.8035} + {0.0387} \times {Rate}
$$

The percentage of variation in {Oxygen} exaplined by {Rate} is given by $R^2=0.9385$ or $93.85\%$. The test of the {Rate} coefficient is highly statistically significant, indicating evidence against a no linear relationship (i.e.\ slope not zero).


## Question 3 Follow the Bouncing Ball: Predictor-Response Data 

Open the file *balldrop.csv*. The problem of interest in this task is the relationship between the time taken for a tennis ball to bounce three times and the height from which the ball was dropped.

(a). What is the explanatory variable and what is the response variable in this experiment?

The explanatory variable is the height at which the ball is dropped.

The response variable is the time from when the ball is dropped to when it hits the ground for the third time.

(b). Form a scatterplot of the time against drop height. Interpret.

```{r}
balldrop=read.csv("balldrop.csv")
View(balldrop)
plot(time~height, data=balldrop, xlab="Height", ylab="Time")
```

Clearly there is a quite linear form, in positive direction with a strong relationship between time and drop height. 


(c). Fit the least squares line to the data using R. Explain the fitted model, by interpreting the slope and intercept.

```{r}
regress.balldrop=lm(time~height, data=balldrop)
summary(regress.balldrop)
plot(time~height, data=balldrop, xlab="Height", ylab="Time")
abline(regress.balldrop, lwd = 3, col = "darkorange") ## adding the fitted line from regress.baldrop
```

The two numbers which define the regression model are given as

time = 0.6392 + 0.7484 height.

*Slope interpretation* $\hat{\beta}_1=0.7484.$  This means that the time for this particular tennis ball to bounce three times increases 0.7484 seconds on average for every 1m increase in the height from which it is dropped. The constant term suggests that when the drop height is 0m the time to the third bounce can be expected to be 0.6392 seconds. 

*Intercept interpretation* $\hat{\beta}_0=0.6392.$ Clearly, if the ball is already on the ground, it makes no sense to talk about the time to the third bounce. Even if this was a sensible concept, a height of 0m is outside the range of the experimental data so extrapolating the model to this value is unlikely to yield an accurate estimate of the true time. Even when this number doesn't have a sensible interpretation in the context of the particular example, it always has meaning in fixing the location of the fitted regression line.

(d). What does the relationship appear to be between time and drop height? 

There is perhaps a suggestion of a curved relationship rather than a strictly linear relationship.

## Question 4. Wet Weight of Maize Plants

In an experiment to investigate the effect of fertiliser on the
growth of maize plants, 60 maize plants were grown, each with
a recorded weight in mg of fertiliser applied.  
The plants were harvested and the wet weight of each in grams was also recorded.  The data were collected and are available in *maize.csv*.

You are to investigate the relationship between the wet weight of a maize plant and the weight of fertiliser applied.

(a). Open the Excel file *maize.csv*;

```{r}
maize=read.csv("maize.csv")
View(maize)
```

There are n=60 observations. 

The explanatory variable is the weight of fertiliser applied.

The response variable is the the wet weight of a maize plant.

(b). Construct a scatterplot of plant weight against fertiliser amount. Observe that the relationship is approximately linear and homoscedastic;

```{r}
plot(Plant.weight.in.g ~ Fertiliser.weight.in.mg, data=maize, xlab="Fertilizer amount", ylab="Weight")
```


(c). Fit the least squares line to the data using R. Explain the fitted model, by interpreting the slope and intercept.

```{r}
maize.reg=lm(Plant.weight.in.g ~ Fertiliser.weight.in.mg, data=maize)
names(maize.reg) ## to check the output
summary(maize.reg)
plot(Plant.weight.in.g ~ Fertiliser.weight.in.mg, data=maize, xlab="Fertilizer amount", ylab="Weight")
abline(maize.reg, lwd = 3, col = "darkorange") ##  to add lines of the form a+bx to a plot.
```


```{r}
maize.reg$coefficients
maize.reg$residual[1:5] # first five values of residuals
maize.reg$fitted.values[1:5] # first five fitted values
```


The fitted model is 

     Plant weight (g) = 7.4020 + 0.021 Fertiliser amount (mg)

*Slope interpretation* $\hat{\beta}_1=0.021.$ For every 1mg increase in amount of fertiliser being applied, the increase in
average plant weight is around 0.021g (slope or gradient).

*Intercept interpretation* $\hat{\beta}_0=7.4020.$According to our model, the average weight of plants grown with no fertiliser
is around 7.4020g (intercept).

(d). Perform a hypothesis testing for a positive slope at a significance level of 5%.

6 steps hypothesis testing for $\beta_1$:

1. $H_0: \beta_1 = 0, H_1:\beta_1 > 0$ 

2. The test statistic (from R output as in the above): 

$t=\frac{\hat{\beta}_1-0}{se(\hat{\beta}_1)}=\frac{0.021060}{0.004404}=4.782$

3. The sampling distribution for the test statistic is $t$ is $T_{df=(n-2)}$
that is $T_{df=60-2=58.}$

4. The p-value = $P(T_{df=58} > 4.782)= 6.148683e-06$ (see below)

```{r}
pval=pt(4.782,58, lower.tail=F)
pval
```

Note that the p-value that being provided in the R output is for two-sided test which is  1.23e-05. For one sided, the p-value is a half of two sided.

5. Decision. Given the p-value is very small, less than the significance level, then we strongly reject the null hypothesis.

6. Conclusion. We conclude that the slope is statistically significantly positive.


(e). Calculate the 95% confidence intervals for both parameters slope and intercept respectively.

```{r}
tval=qt(0.025, 58, lower.tail=F)
tval
```


Use the following formulas to calculate the confidence intervals for $\beta_1, \beta_0$ respectively:

$$\hat{\beta}_1 \pm t_{df=58, 0.025} se(\hat{\beta}_1)=0.02106\pm 2.0017(0.0044)=(0.0122, 0.0299)$$
```{r}
se.beta1=0.0044
upper.beta1=maize.reg$coefficients[2]+tval*se.beta1  ## slope
upper.beta1
lower.beta1=maize.reg$coefficients[2]-tval*se.beta1
lower.beta1
```

$$\hat{\beta}_0 \pm t_{df=58, 0.025} se(\hat{\beta}_0)=7.40199 \pm 2.0017(0.8622)=(5.6761,9.1279)$$
```{r}
se.beta0=0.8622
upper.beta0=maize.reg$coefficients[1]+tval*se.beta0
upper.beta0
lower.beta0=maize.reg$coefficients[1]-tval*se.beta0
lower.beta0
```

This R command *confit* provides you with the results.

```{r}
confint(maize.reg)
```

The 95% confidence intervals for the slope ${\beta}_1$:

$$ 0.0122 < {\beta}_1 < 0.0299.$$
The interpretation: we are 95% confident that the population slope of this context is between 0.0122 and 0.0299. This confirms the hypothesis testing in the above that 0 is not within the interval, ie the slope is positive.

The 95% confidence intervals for the intercept ${\beta}_0$:

$$ 5.6760 < {\beta}_0 < 9.1280.$$

We are 95% confident that the population intercept of this context is between 5.6760 and 9.1280.

## Question 5  Revisiting Geyser data

A geyser is a hot spring in which water boils intermittently, sending a tall column of water and steam into the air. One of the best known geysers in the US is Old Faithful in Yellowstone National Park, Wyoming. It was so named when it was first discovered in 1870 because it was (and is) said to erupt regularly. But what does regularly mean?

The data file for this workshop contains the _R_ data frame `faithful`, which consists of sequential measurements of the duration of an eruption (eruptions, in minutes) and the time to the next eruption (waiting, also in minutes). They were obtained during October 1980 by volunteers. These data were collected so that park rangers could inform tourists when an eruption was likely to occur.

As part of your work with the US National Park Service, you have been tasked with constructing a predictive model for waiting time as a linear function of eruption duration so that park rangers can inform tourists when an eruption is likely to occur.

(a) Produce a scatterplot of the waiting time and length of eruptions. 
Comment on any patterns you see in the data and on the relationship between waiting time and eruption duration in such a way that a tourist to Old Faithful might understand. 

```{r}
View(OldFaithful)
head(OldFaithful)
plot(waiting ~ eruptions, data = OldFaithful, xlab = "eruption duration (minutes)",
ylab = "time to next eruption (minutes)", main = "Eruptions from Old Faithful (October 1980)")
```

n=272

The interpretations:

1. (*Form, direction*). There is a positive linear relationship between eruption duration and the time till the next one. This means that the longer the eruption, the longer you have to wait till the next one. 

2. (*Strength*) However, it appears this relationship is due to some extent to clustering of the eruptions into 2 main groups: those with duration time less than about 2.5 minuteswith waiting time less than 70 minutes, and those lasting longer than 3.5 minutes with waiting time greater than 70 minutes. 

3. After an eruption of a certain duration, there is a range of waiting times.

(b). Fit the least squares line to the data using R. Explain the fitted model, by interpreting the slope and intercept.

```{r}
geyser.lm=lm(waiting ~ eruptions, data = OldFaithful)
summary(geyser.lm)
plot(waiting ~ eruptions, data = OldFaithful, xlab = "eruption duration (minutes)",
ylab = "time to next eruption (minutes)", main = "Eruptions from Old Faithful (October 1980)")
abline(geyser.lm, lwd = 3, col = "darkorange")
```

The fitted model is 

    Waiting time = 33.4744 + 10.7296 (eruptions duration)

*Slope interpretation* $\hat{\beta}_1= 10.7296.$ For every 1-minute increase in duration of an eruption there will be a mean increase in waiting time until the next one of 10.73 minutes.

*Intercept interpretation* $\hat{\beta}_0=33.4744.$ According to our model, the average of waiting time until the next one when there is no eruption is 33.47 minutes. However, in this context, it is not interpretable.


(c). Perform a hypothesis testing for a positive slope at a significance level of 1%.

6 steps hypothesis testing for $\beta_1$:

1. $H_0: \beta_1 = 0, H_1:\beta_1 > 0$ 

2. The test statistic (from R output as in the above): 

$t=\frac{\hat{\beta}_1-0}{se(\hat{\beta}_1)}=\frac{10.7296}{0.3148}=34.09.$

3. The sampling distribution for the test statistic is $t$ is $T_{df=(n-2)}$
that is $T_{df=272-2=270.}$

4. The p-value = $P(T_{df=270} > 34.09)= 4.03995e-100$ (see below)

```{r}
pval=pt(34.09,270, lower.tail=F)
pval
```


5. Decision. Given the p-value is very small, less than the significance level, then we strongly reject the null hypothesis.

6. Conclusion. We conclude that the slope is statistically significantly positive.

(d). Calculate the 99% confidence intervals for both parameters slope and intercept respectively.

```{r}
confint(geyser.lm, level =0.99)
```

Calculate a 99% confidence interval for the true slope.

```{r}
se.beta1.g=0.3148
se.beta0.g=1.1549
c(geyser.lm$coefficients[2]-qt(0.005,270)*se.beta1.g,geyser.lm$coefficients[2]+qt(0.005,270)*se.beta1.g)
c(geyser.lm$coefficients[1]-qt(0.005,270)*se.beta0.g,geyser.lm$coefficients[1]+qt(0.005,270)*se.beta0.g)
```

We are 99% confident that the population slope of this context is between 9.9129 and 11.5463.


## Question 6 Delivery data

An industrial engineer employed by a soft drink beverage bottler is analyzing
the product delivery and service operations for vending machines. He suspects that
the time (*Time*) required by a route deliveryman to load and service a machine is related to the number of cases (*Volume*) of product delivered. The engineer visits 25 randomly chosen retail outlets having vending machines, and the in-outlet delivery time (*Time*) (in minutes) and the volume of product delivered (*Volume*) (in cases) are observed for each. The data is available in {Delivery.csv}.

a. Fit a simple linear regression model relating delivery time (*Time*)
to the volume of product delivered (*Volume*), that is 
$$
Time = \beta_0 + \beta_1 Volume + \epsilon
$$
where $\epsilon$ is a normal random variable. 

b. Determine the estimate of the regression coefficients  $\beta_0$ and $\beta_1$ and the fitted model.


Read the data in and plot the data

```{r size="scriptsize"}
Delivery = read.csv("Delivery.csv",header=TRUE)
with(Delivery,plot(Volume,Time,type="p"))
```

Fit the regression model and put the fitted line in

```{r size="scriptsize"}
M2 = lm(Time~Volume,data=Delivery)
with(Delivery,plot(Time~Volume,type="p"))
with(Delivery,lines(fitted(M2)~Volume))
```


```{r size="scriptsize"}
summary(M2)
```

Consider the model as 
$$
Time = \beta_0 + \beta_1 Volume + \epsilon
$$
where $\epsilon$ is a normal random variable. So the estimates are 

$$
\hat\beta_0 = 3.321,  \quad \quad
\hat\beta_1 = 2.176
$$
and the fitted line is 
$$
\widehat{Time} =  3.321 +2.176 Volume
$$



c. Test whether the model is statistically significant at the level 5\%.

To test H$_0:\beta_1=0$ vs H$_1:\beta_1\neq 0$, we can look the $p$-value from the output. 
the $p$-value is {8.22e-15} $<0.05=\alpha$, so 
the model is statistically significant at the level 5\%.



d. Determine a 95\% CI for the estimate of the slope.


```{r size="scriptsize"}
confint(M2,level=0.95)
```

The 95\% CI for the estimate of the slope is $(1.9195920,2.432741)$


e. If the volume of product delivered (*Volume*) is 10,
determine a 95\% confidence interval and a 95\% prediction interval for the delivery time (*Time*).


```{r size="scriptsize"}
predict(M2,newdata=data.frame(Volume=10),interval="confidence")
predict(M2,newdata=data.frame(Volume=10),interval="prediction")
```

Given the volume of product delivered (*Volume*) is 10, 
the 95\% CI is $(23.32346,26.84143)$ and the 95\% PI is $(16.25553,33.90936)$
for the delivery time (*Time*).



## Question 7 Concept: manual calculation for hypothesis testing

We are interested in fitting a simple linear regression

$$
y = \beta_0 + \beta_1 x + \epsilon
$$
where $\epsilon$ is normally distributed with mean 0 and variance $\sigma^2$.

Use the following statistics
$\sum_{i=1}^n x_i = 250$,
$\sum_{i=1}^n x_i^2 = 2250$,
$\sum_{i=1}^n y_i^2 = 3546$,
$\sum_{i=1}^n y_i = 350$,
$\sum_{i=1}^n x_iy_i = 2750$, 
and $n=50$,
to determine the estimate of $\sigma^2$, and we are also interested in 
the test H$_0:\beta_1=0$ vs H$_1:\beta_1\neq0$, take the level of significance be $\alpha=0.05$.


First we have the residual sum of squares
$$
\sum_{i=1}^n (y_i-\widehat{y}_i)^2
=
\sum_{i=1}^n (y_i-\overline{y})^2
-
\widehat{\beta}_1^2\sum_{i=1}^n (x_i-\overline{x})^2
$$
We first compute 
$$
\overline{x} = \frac{1}{n}\sum_{i=1}^n x_i = \frac{250}{50} = 5\\
\overline{y} = \frac{1}{n}\sum_{i=1}^n y_i = \frac{350}{50} = 7\\
\sum_{i=1}^n (y_i-\overline{y})^2 
=
\sum_{i=1}^n y_i^2- n\overline{y}^2
= 3546 - 50\times 7^2 = 1096
\\
\sum_{i=1}^n (x_i-\overline{x})^2 
=
\sum_{i=1}^n x_i^2- n\overline{x}^2
= 2250- 50\times 5^2 = 1000
\\
\sum_{i=1}^n (x_i-\overline{x})(y_i-\overline{y})
=
\sum_{i=1}^n x_iy_i- n\overline{x}\overline{y}
= 2750- 50\times 5\times 7 = 1000
$$

So we have
$$
\widehat{\beta}_1 = \frac{\sum_{i=1}^n (x_i-\overline{x})(y_i-\overline{y})}{\sum_{i=1}^n (x_i-\overline{x})^2} = \frac{1000}{1000} = 1
$$

and the residual sum of squares is
$$
\sum_{i=1}^n (y_i-\widehat{y}_i)^2
=
\sum_{i=1}^n (y_i-\overline{y})^2
-
\widehat{\beta}_1^2\sum_{i=1}^n (x_i-\overline{x})^2
=1096-
1^2\times1000=96
$$
and so
$$
\widehat{\sigma}^2 = \frac{1}{n-2}\sum_{i=1}^n (y_i-\widehat{y}_i)^2
= \frac{96}{50-2}=2
$$


Here we also know that 

$$
\widehat{Var}(\widehat{\beta}_1)
=\frac{\widehat{\sigma}^2}{\sum_{i=1}^n (x_i-\overline{x})^2}
=\frac{2}{1000} = 0.002
$$

so, the t-statistic for the test H$_0:\beta_1=0$ vs H$_1:\beta_1\neq0$ is

$$
\frac{\widehat{\beta}_1}{\sqrt{\widehat{Var}(\widehat{\beta}_1)}}
=\frac{1}{\sqrt{0.002}}=22.36068
$$

The $p$-value is then given by

```{r size="scriptsize"}
tobs = 1/sqrt(0.002)
tobs
2*(1-pt(abs(tobs),50-2))
```

So $p$-value $<0.05$ and we  reject H$_0$ at the 5\% significance level.

Or we could use the $t$-critical value

```{r size="scriptsize"}
qt(1-0.05/2,50-2)
```

so we reject H$_0$ if the $t$-statistic is greater than 
$2.010635$ or less than $-2.010635$, again we  reject H$_0$ at the 5\% significance level.



